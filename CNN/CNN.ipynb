{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[{"file_id":"1htiIQd1-fbh8OTc-v9EzHa2lsHrWhJ_t","timestamp":1573359248790}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yRoKD9e4SiNA","colab_type":"code","outputId":"dda3e2d4-d2a0-4d5b-b473-5e9764ae3fba","executionInfo":{"status":"ok","timestamp":1573472899558,"user_tz":-330,"elapsed":4715,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pip install -U PyYAML"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: PyYAML in /usr/local/lib/python3.6/dist-packages (5.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zjGkTUTaFLwp","colab_type":"code","outputId":"527ef9ed-a01e-4c13-f3ba-88fd3dc6fabd","executionInfo":{"status":"ok","timestamp":1573488952741,"user_tz":-330,"elapsed":765,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import pandas as pd\n","import numpy as np\n","import string\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import KFold\n","from keras.utils.np_utils import to_categorical\n","from keras.models import load_model\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import classification_report\n","import tensorflow as tf\n","\n","\n","\n","from tqdm import tqdm\n","\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","# from gensim.models import Word2Vec\n","# from gensim.models import KeyedVectors\n","import pickle\n","\n","import os\n","\n","from collections import Counter\n","from scipy.sparse import hstack\n","\n","from prettytable import PrettyTable\n","import warnings\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import PorterStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kyb3lV88HQ80","colab_type":"code","outputId":"f6f43ea7-fd2d-4496-85c6-e298327aecbb","executionInfo":{"status":"ok","timestamp":1573488953202,"user_tz":-330,"elapsed":545,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = '/content/gdrive/My Drive/IRE_Major_Project/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wX8EVgV-C_UJ","colab_type":"code","colab":{}},"source":["def decontracted(phrase):\n","    # specific\n","    phrase = re.sub(r\"won't\", \" will not \", phrase)\n","    phrase = re.sub(r\"can\\'t\", \" can not \", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not \", phrase)\n","    phrase = re.sub(r\"\\â€™re\", \" are \", phrase)\n","    phrase = re.sub(r\"\\â€™s\", \" is \", phrase)\n","    phrase = re.sub(r\"\\â€™d\", \" would \", phrase)\n","    phrase = re.sub(r\"\\â€™ll\", \" will \", phrase)\n","    phrase = re.sub(r\"\\â€™t\", \" not \", phrase)\n","    phrase = re.sub(r\"\\â€™ve\", \" have \", phrase)\n","    phrase = re.sub(r\"\\â€™m\", \" am \", phrase)\n","    return phrase\n","\n","def remove_line_breaks(text):\n","  text = text.replace('\\\\r', ' ')\n","  text = text.replace('\\\\\"', ' ')\n","  text = text.replace('\\\\n', ' ')\n","  return text\n","\n","  # https://gist.github.com/sebleier/554280\n","# we are removing the words from the stop words list: 'no', 'nor', 'not'\n","stopwords= { 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n","            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n","            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n","            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n","            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n","            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n","            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n","            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n","            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n","            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n","            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n","            'won', \"won't\", 'wouldn', \"wouldn't\",\"url\",\"user\"}\n","\n","def perform_cleaning(text):\n","  phrase = text.lower().strip()\n","  # print(phrase)\n","  phrase = ' '.join(e for e in phrase.split() if e not in stopwords)\n","  phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase)\n","  if not phrase:\n","    phrase = text.lower().strip()\n","  return phrase\n","\n","def do_tokenize_lemmatize(text):\n","  lemmatizer = WordNetLemmatizer().lemmatize\n","\n","  phrase = \" \".join(lemmatizer(x,pos=\"v\") for x in word_tokenize(text))\n","  if not phrase:\n","    phrase = text\n","  return phrase\n","\n","def remove_user(text):\n","  return re.sub(\"@USER\", '', text)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHAxIYLGDAe-","colab_type":"code","colab":{}},"source":["def get_user_mentions(text):\n","  return len(re.findall(\"@USER\", text))\n","\n","def remove_user(text):\n","  return re.sub(\"@USER\", '', text)\n","\n","def get_hash_tags(text):\n","  return len(re.findall(\"#\", text))\n","\n","\n","def get_url_count(text):\n","  return len(re.findall(\"URL\",text))\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-NH3SX9FXE9","colab_type":"code","colab":{}},"source":["data = pd.read_csv(root_path+'preprocessed.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkPgCn9mHngE","colab_type":"code","outputId":"ede5bbf0-5580-41ac-8c6a-cd0a6aec132d","executionInfo":{"status":"ok","timestamp":1573488955566,"user_tz":-330,"elapsed":621,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["\n","data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>new_tweet</th>\n","      <th>user_mentions</th>\n","      <th>n_hash_tags</th>\n","      <th>n_urls</th>\n","      <th>n_emojis</th>\n","      <th>subtask_a</th>\n","      <th>subtask_b</th>\n","      <th>subtask_c</th>\n","      <th>original_tweet_length</th>\n","      <th>new_tweet_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>86426</td>\n","      <td>@USER She should ask a few native Americans wh...</td>\n","      <td>she ask native americans their take be</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>OFF</td>\n","      <td>UNT</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>90194</td>\n","      <td>@USER @USER Go home youâ€™re drunk!!! @USER #MAG...</td>\n","      <td>go home you drink maga trump2020</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>OFF</td>\n","      <td>TIN</td>\n","      <td>IND</td>\n","      <td>11</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16820</td>\n","      <td>Amazon is investigating Chinese employees who ...</td>\n","      <td>amazon investigate chinese employees sell inte...</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>NOT</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>27</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>62688</td>\n","      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n","      <td>someone should vetaken piece shit volcano</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>OFF</td>\n","      <td>UNT</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>43605</td>\n","      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n","      <td>obama want liberals amp illegals move red state</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NOT</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>12</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ... new_tweet_length\n","0  86426  ...                7\n","1  90194  ...                6\n","2  16820  ...               19\n","3  62688  ...                6\n","4  43605  ...                8\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":168}]},{"cell_type":"markdown","metadata":{"id":"6XsMzgaxB6Wm","colab_type":"text"},"source":["Dropping subtask a and c columns"]},{"cell_type":"markdown","metadata":{"id":"b_yf8Jzrx0Yo","colab_type":"text"},"source":["# Splitting Data: Train and Test\n"]},{"cell_type":"code","metadata":{"id":"df_vlf3_x5Ac","colab_type":"code","outputId":"826e6ba7-22e1-47b2-cd96-1453499ca695","executionInfo":{"status":"ok","timestamp":1573488960219,"user_tz":-330,"elapsed":769,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Y = data[['subtask_b'  , 'subtask_a' ,  'subtask_c']]\n","X = data.drop(['subtask_b' , 'subtask_a' , \"subtask_c\" ,'id'],axis=1)\n","print(\"Shape of X: \",X.shape)\n","print(\"Shape of Y: \",Y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of X:  (13240, 8)\n","Shape of Y:  (13240, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B9wUVS59GHb6","colab_type":"text"},"source":["#### Change the mapping of the label column to binary\n"]},{"cell_type":"code","metadata":{"id":"DCGIj0WKFToi","colab_type":"code","colab":{}},"source":["Y['subtask_a'] = Y['subtask_a'].map({'OFF' : 'OFFENIVE' , 'NOT' : 'NOT OFFENSIVE'})\n","Y['subtask_b'] = Y['subtask_b'].map({'UNT' : 'NOT TARGATED' , 'TIN' : 'TARGATED'})\n","Y['subtask_c'] = Y['subtask_c'].map({'OTH' : 'OTHER' , 'IND' : 'INDIVIDUAL' , 'GRP' : \"GROUP\"})\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5BIuG8vK5Uq","colab_type":"code","colab":{}},"source":["# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n","#     print(X_train.isnull())\n","#  X_train[X_train.isna().any(axis=1)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjrCkgo0j4yC","colab_type":"text"},"source":["# Convolution Model"]},{"cell_type":"code","metadata":{"id":"tjBDKSO9j_CI","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras.optimizers import SGD, Adam\n","from keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from keras.initializers import RandomNormal\n","from keras.engine import Layer, InputSpec\n","from keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKgmZMCRkHzZ","colab_type":"text"},"source":["## Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"krnQBIu6xUyn","colab_type":"text"},"source":["## Making Data Model Ready: Encoding Tweets"]},{"cell_type":"markdown","metadata":{"id":"Ba4weCmHlNgw","colab_type":"text"},"source":["### Quantization  "]},{"cell_type":"code","metadata":{"id":"basXFKCelM4e","colab_type":"code","colab":{}},"source":["def encode_data(x, maxlen, vocab):\n","    # Iterate over the loaded data and create a matrix of size (len(x), maxlen)\n","    # Each character is encoded into a one-hot array later at the lambda layer.\n","    # Chars not in the vocab are encoded as -1, into an all zero vector.\n","    \n","    input_data = np.zeros((len(x), maxlen), dtype=np.int)\n","    for dix, sent in enumerate(x):\n","        counter = 0\n","        for c in sent:\n","            if counter >= maxlen:\n","                pass\n","            else:\n","                ix = vocab.get(c, -1)  # get index from vocab dictionary, if not in vocab, return -1\n","                input_data[dix, counter] = ix\n","                counter += 1\n","    return input_data\n","\n","\n","def create_vocab_set():\n","    # This alphabet is 69 chars vs. 70 reported in the paper since they include two\n","    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n","\n","    alphabet = set(list(string.ascii_lowercase) + list(string.digits) +\n","                   list(string.punctuation) + ['\\n'])\n","    vocab_size = len(alphabet)\n","    vocab = {}\n","    reverse_vocab = {}\n","    for ix, t in enumerate(alphabet):\n","        vocab[t] = ix\n","        reverse_vocab[ix] = t\n","\n","    return vocab, reverse_vocab, vocab_size, alphabet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzh6vExltua","colab_type":"code","colab":{}},"source":["vocab, reverse_vocab, vocab_size, alphabet = create_vocab_set()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMg7jWzWrsyP","colab_type":"text"},"source":["## feature set"]},{"cell_type":"code","metadata":{"id":"XtvD_NBu92Y9","colab_type":"code","outputId":"ac1754e8-5c4c-428f-9e98-20aff0845ec6","executionInfo":{"status":"ok","timestamp":1573489008213,"user_tz":-330,"elapsed":1932,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["f11 = encode_data(X['new_tweet'], 1500 , vocab)\n","f12 = encode_data(X['new_tweet'], 1014 , vocab)\n","f13 = encode_data(X['new_tweet'], 1200 , vocab)\n","\n","f2 = X['user_mentions'].values.reshape(-1,1)\n","f3 = X['n_hash_tags'].values.reshape(-1,1)\n","f4 = X['n_urls'].values.reshape(-1,1)\n","f5 = X['n_emojis'].values.reshape(-1,1)\n","f6 = X['new_tweet_length'].values.reshape(-1,1)\n","f7 = X['original_tweet_length'].values.reshape(-1,1)\n","\n","X_encoded1 = np.concatenate((f11 , f2 , f3 , f4 , f7 ) , axis = 1)\n","X_encoded2 = np.concatenate((f12 , f2 , f3 , f4 , f5 , f6 , f7 ) , axis = 1)\n","X_encoded3 = np.concatenate((f13 , f2 , f3 , f4 , f6 ) , axis = 1)\n","\n","print(X_encoded1.shape)\n","print(X_encoded2.shape)\n","print(X_encoded3.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(13240, 1504)\n","(13240, 1020)\n","(13240, 1204)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CVt5ml5wROAO","colab_type":"code","outputId":"2c0f734b-1e2f-492a-b478-469705ce3482","executionInfo":{"status":"ok","timestamp":1573489285814,"user_tz":-330,"elapsed":850,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["print(f11)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[62 35 27 ...  0  0  0]\n"," [13 66 -1 ...  0  0  0]\n"," [ 0 14  0 ...  0  0  0]\n"," ...\n"," [56 27 43 ...  0  0  0]\n"," [43 11 62 ...  0  0  0]\n"," [62 43  0 ...  0  0  0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hLY5_ACyRUns","colab_type":"code","outputId":"b907dc24-48ff-46b8-84dd-f90aa08c9d3c","executionInfo":{"status":"ok","timestamp":1573489313927,"user_tz":-330,"elapsed":966,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["print(X_encoded1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[62 35 27 ...  0  0 14]\n"," [13 66 -1 ...  2  1 11]\n"," [ 0 14  0 ...  5  1 27]\n"," ...\n"," [56 27 43 ...  0  0 11]\n"," [43 11 62 ...  0  0  2]\n"," [62 43  0 ...  9  1 30]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1500nfeuNz3m","colab_type":"code","outputId":"fcbfad61-b357-46b4-fe51-cd4479595a6a","executionInfo":{"status":"ok","timestamp":1573489107854,"user_tz":-330,"elapsed":89224,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## Loading all saved model\n","\n","from keras.models import model_from_yaml\n","# import yaml\n","\n","# from keras.models import load_model\n","\n","\n","yaml_file1 = open(root_path + '/CNN/model_A.yaml', 'r')\n","loaded_model_yaml1 = yaml_file1.read()\n","# yaml_file1.close()\n","model1 = model_from_yaml(loaded_model_yaml1)\n","# load weights into new model\n","model1.load_weights(root_path + \"/CNN/model_A.h5\")\n","\n","\n","yaml_file2 = open(root_path + '/CNN/model_B.yaml', 'r')\n","loaded_model_yaml2 = yaml_file2.read()\n","yaml_file2.close()\n","\n","model2 = model_from_yaml(loaded_model_yaml2)\n","# load weights into new model\n","model2.load_weights(root_path + \"/CNN/model_B.h5\")\n","\n","\n","yaml_file3 = open(root_path + '/CNN/model_C.yaml', 'r')\n","loaded_model_yaml3 = yaml_file3.read()\n","yaml_file3.close()\n","model3 = model_from_yaml(loaded_model_yaml3)\n","# load weights into new model\n","model3.load_weights(root_path + \"/CNN/model_C.h5\")\n","print(\"Loaded model from disk\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded model from disk\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OZ8myYNisCAm","colab_type":"text"},"source":["# Predictions"]},{"cell_type":"code","metadata":{"id":"BPqhLRltZ5xp","colab_type":"code","colab":{}},"source":["def predict(sample1, sample2 , sample3) :\n","  res1 = model1.predict(sample1)\n","  res1_cat = res1.argmax(axis = 1)\n","\n","  if res1_cat[0] == 0 :\n","    print(\"Subtask A Result :  NOT OFFENSIVE\")\n","    return\n","\n","  res2 = model2.predict(sample2)\n","  res2_cat = res2.argmax(axis = 1)\n","\n","  if res2_cat[0] == 0 :\n","    print(\"Subtask A Result : OFFENSIVE\")\n","    print(\"Subtask B Result : NOT TARGETED\")\n","    return \n","\n","  res3 = model3.predict(sample3)\n","  res3_cat = res3.argmax(axis = 1)\n","\n","  print(\"Subtask A Result : OFFENSIVE\")\n","  print(\"Subtask B Result : TARGETED\")\n","  if res3_cat[0] == 0 : \n","    print(\"Subtask C Result : OTHER\")\n","  elif res3_cat[0] == 1 :\n","    print(\"Subtask C Result : INDIVIDUAL\")\n","  else :\n","    print(\"Subtask C Result : GROUP\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CXEgBg68YpZB","colab_type":"code","colab":{}},"source":["import re\n","def unseen_predict(tweet) :\n","  new_tweet = decontracted(tweet.lower())\n","  new_tweet = remove_line_breaks(new_tweet)\n","  new_tweet = remove_user(new_tweet)\n","  new_tweet = perform_cleaning(new_tweet)\n","  new_tweet = do_tokenize_lemmatize(new_tweet)\n","\n","  n_tweet = [new_tweet]\n","  f2 = [[get_user_mentions(tweet)]]\n","  f3 = [[get_hash_tags(tweet)]]\n","  f4 = [[get_url_count(tweet)]]\n","  f5 =[[0]]\n","  f6 = [[len(new_tweet.split())]]\n","  f7 = [[len(tweet.lower().split())]]\n","  # print(new_tweet)\n","  x = encode_data(n_tweet, 1500 , vocab)\n","  encoding1 = np.concatenate((x , f2 , f3 , f4 , f7) , axis = 1) \n","  # print(x)\n","  encoding2 = np.concatenate((encode_data(n_tweet, 1014 , vocab) , f2 , f3 , f4 , f5 ,f6 , f7) , axis = 1) \n","  encoding3 = np.concatenate((encode_data(n_tweet, 1200 , vocab) , f2 , f3 , f4 , f6) , axis = 1) \n","\n","\n","  predict(encoding1 , encoding2 , encoding3)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kaDIVr0TdlaW","colab_type":"text"},"source":["## FINAL PREDICTIONS"]},{"cell_type":"code","metadata":{"id":"LfgHBF4NW_Uz","colab_type":"code","outputId":"a0a31ac3-2f70-4b4b-d523-e98623b0b200","executionInfo":{"status":"ok","timestamp":1573500598839,"user_tz":-330,"elapsed":786,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":392}},"source":["for ind in range(1600,1602):\n","  print(\"tweet is :  \",  X[\"tweet\"][ind] )\n","\n","  print(\"\\n\\npredictions are :  \\n\")\n","  predict(X_encoded1[ind:ind+1] , X_encoded2[ind:ind+1] , X_encoded3[ind:ind+1] )\n","\n","  print(\"\\n\\nOriginal Labels are  : \\n\")\n","  # print(Y.iloc[ind , :])\n"],"execution_count":276,"outputs":[{"output_type":"stream","text":["tweet is :   @USER She donâ€™t even wear the shit I buy her now. Move away from me.\n","\n","\n","predictions are :  \n","\n","Subtask A Result :  NOT OFFENSIVE\n","\n","\n","Original Labels are  : \n","\n","tweet is :   @USER @USER Chequers delivers Brexit (changes the relationship with the EU). It does not deliver leave the European Union\" (referendum question.) I think we should stop using the word Brexit to mean leave, and actually say \"leave the EU\". \"Brexit\" meaning willfully mutated by remainers.\"\n","\n","\n","predictions are :  \n","\n","Subtask A Result :  NOT OFFENSIVE\n","\n","\n","Original Labels are  : \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5x3W6Wz29zF8","colab_type":"text"},"source":["# LIVE DEMO"]},{"cell_type":"code","metadata":{"id":"zTT2Wbf-A-WY","colab_type":"code","outputId":"65105cb1-7a2d-48d3-80f6-cf6211810202","executionInfo":{"status":"ok","timestamp":1573501319570,"user_tz":-330,"elapsed":762,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# tweet = \"You're another hypocrite\"\n","# tweet = \"he is useless\"\n","# tweet = \"@USER I suppose you think the way liberals attacked Sarah Palin was courageous also. Youâ€™re just another hypocrite.\"\n","# tweet = \"@USER Bravo. Down with CNN.\"\n","# tweet = \"@USER Never nigga ðŸ’€ðŸ’€\"\n","# tweet = \"@USER @USER Love you Norm!\"\n","unseen_predict(tweet)"],"execution_count":283,"outputs":[{"output_type":"stream","text":["Subtask A Result :  NOT OFFENSIVE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5Yu7GJH665lS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}