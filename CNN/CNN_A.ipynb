{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_A.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zjGkTUTaFLwp","colab_type":"code","outputId":"8b214dda-0329-41f3-c936-c212d43dd356","executionInfo":{"status":"ok","timestamp":1573484438078,"user_tz":-330,"elapsed":3489,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["import pandas as pd\n","import numpy as np\n","import string\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import KFold\n","from keras.utils.np_utils import to_categorical\n","from keras.models import load_model\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import classification_report\n","\n","\n","from tqdm import tqdm\n","\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","# from gensim.models import Word2Vec\n","# from gensim.models import KeyedVectors\n","import pickle\n","\n","import os\n","\n","from collections import Counter\n","from scipy.sparse import hstack\n","\n","from prettytable import PrettyTable\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Kyb3lV88HQ80","colab_type":"code","outputId":"1386ef50-5a68-486a-9741-fba54d3f11d9","executionInfo":{"status":"ok","timestamp":1573484463164,"user_tz":-330,"elapsed":21774,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = '/content/gdrive/My Drive/IRE_Major_Project/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i-NH3SX9FXE9","colab_type":"code","colab":{}},"source":["data = pd.read_csv(root_path+'preprocessed.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkPgCn9mHngE","colab_type":"code","outputId":"49a3dbaa-1740-45a9-aa22-21cb8d68732b","executionInfo":{"status":"ok","timestamp":1573484464452,"user_tz":-330,"elapsed":3769,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["data.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>new_tweet</th>\n","      <th>user_mentions</th>\n","      <th>n_hash_tags</th>\n","      <th>n_urls</th>\n","      <th>n_emojis</th>\n","      <th>subtask_a</th>\n","      <th>subtask_b</th>\n","      <th>subtask_c</th>\n","      <th>original_tweet_length</th>\n","      <th>new_tweet_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>86426</td>\n","      <td>@USER She should ask a few native Americans wh...</td>\n","      <td>she ask native americans their take be</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>OFF</td>\n","      <td>UNT</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>90194</td>\n","      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n","      <td>go home you drink maga trump2020</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>OFF</td>\n","      <td>TIN</td>\n","      <td>IND</td>\n","      <td>11</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16820</td>\n","      <td>Amazon is investigating Chinese employees who ...</td>\n","      <td>amazon investigate chinese employees sell inte...</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>NOT</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>27</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>62688</td>\n","      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n","      <td>someone should vetaken piece shit volcano</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>OFF</td>\n","      <td>UNT</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>43605</td>\n","      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n","      <td>obama want liberals amp illegals move red state</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NOT</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>12</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ... new_tweet_length\n","0  86426  ...                7\n","1  90194  ...                6\n","2  16820  ...               19\n","3  62688  ...                6\n","4  43605  ...                8\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"BNnTob5KASY7","colab_type":"code","colab":{}},"source":["data.drop(columns = ['subtask_b' , 'subtask_c'] , inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_yf8Jzrx0Yo","colab_type":"text"},"source":["# Splitting Data: Train and Test\n"]},{"cell_type":"code","metadata":{"id":"-wC1MiOryP9z","colab_type":"code","outputId":"95142e76-1b6a-411e-857b-a93671e27358","executionInfo":{"status":"ok","timestamp":1573484469541,"user_tz":-330,"elapsed":746,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["data['subtask_a'].value_counts()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NOT    8840\n","OFF    4400\n","Name: subtask_a, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"df_vlf3_x5Ac","colab_type":"code","outputId":"89504432-e8c6-4ecf-c54a-be5a4fb0c033","executionInfo":{"status":"ok","timestamp":1573484470085,"user_tz":-330,"elapsed":1118,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Y = data['subtask_a']\n","X = data.drop(['subtask_a','id'],axis=1)\n","print(\"Shape of X: \",X.shape)\n","print(\"Shape of Y: \",Y.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Shape of X:  (13240, 8)\n","Shape of Y:  (13240,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z5JF2kMeyg5W","colab_type":"text"},"source":["#### Observation: \n","We see that the dataset is clearly imbalanced with NOT OFFENSIVE tweets being the majority class.\n"]},{"cell_type":"code","metadata":{"id":"ZvfvXPtG0kiE","colab_type":"code","outputId":"68904bea-ec09-4355-c4e8-743f7fe55567","executionInfo":{"status":"ok","timestamp":1573484470621,"user_tz":-330,"elapsed":822,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["#separating data into train and test\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.30,stratify=Y,random_state=42)\n","print(\"Shape of X_train: \", X_train.shape)\n","print(\"Shape of Y_train: \",Y_train.shape)\n","print(\"Shape of X_test: \",X_test.shape)\n","print(\"Shape of Y_test: \",Y_test.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Shape of X_train:  (9268, 8)\n","Shape of Y_train:  (9268,)\n","Shape of X_test:  (3972, 8)\n","Shape of Y_test:  (3972,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRkiiriWKUdp","colab_type":"code","outputId":"bdd0338f-68b3-4a86-e8d0-5eb1721108e7","executionInfo":{"status":"ok","timestamp":1573484471294,"user_tz":-330,"elapsed":1073,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["X_train['new_tweet'].head()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8902     1965 immigration act work exactly liberals hop...\n","10197                     interest you your state illinois\n","6956     people tire virtue signal wealthy liberals nev...\n","9853     i think you say write intelligent commentary h...\n","11952           she biggest thing she fail everywhere else\n","Name: new_tweet, dtype: object"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"B9wUVS59GHb6","colab_type":"text"},"source":["#### Change the mapping of the label column to binary\n"]},{"cell_type":"code","metadata":{"id":"DCGIj0WKFToi","colab_type":"code","colab":{}},"source":["Y_train = Y_train.map(dict(OFF=1, NOT=0))\n","Y_test = Y_test.map(dict(OFF=1, NOT=0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5BIuG8vK5Uq","colab_type":"code","colab":{}},"source":["# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n","#     print(X_train.isnull())\n","#  X_train[X_train.isna().any(axis=1)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xZRQHuliEOs","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras.optimizers import SGD, Adam\n","from keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from keras.initializers import RandomNormal\n","from keras.engine import Layer, InputSpec\n","from keras import backend as K\n","\n","def recall_m(y_true, y_pred):  \n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","def precision_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","def f_m(y_true, y_pred):\n","    tmp = precision_recall_fscore_support(y_true , y_pred)\n","    return tmp[2][1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYitMGQmJWFj","colab_type":"text"},"source":["## creating model"]},{"cell_type":"code","metadata":{"id":"EVzFOBc05Tt-","colab_type":"code","colab":{}},"source":["def create_model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output):\n","    initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n","\n","    # Define what the input shape looks like\n","    inputs = Input(shape=(maxlen,), dtype='int64')\n","    print('inputs shape: ',inputs.shape)\n","    a_func = 'relu'\n","    import tensorflow as tf\n","\n","    def one_hot(x):\n","        return tf.one_hot(x, vocab_size, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32)\n","\n","    def one_hot_outshape(in_shape):\n","        return in_shape[0], in_shape[1], vocab_size\n","\n","    embedded = Lambda(one_hot, output_shape=one_hot_outshape)(inputs)\n","\n","    # All the convolutional layers...\n","    conv = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0], kernel_initializer=initializer, \n","                         padding='valid', activation = a_func,\n","                         input_shape=(maxlen, vocab_size), name='Conv1')(embedded)\n","    print('conv shape: ',conv.shape)\n","    conv = MaxPooling1D(pool_size=2, strides = 2, name='MaxPool1')(conv)\n","    print('conv max pool shape: ',conv.shape)\n","\n","    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1], kernel_initializer=initializer, \n","                          padding='valid', activation=a_func, name='Conv2')(conv)\n","    print('conv1 shape: ',conv1.shape)\n","\n","    # conv1 = MaxPooling1D(pool_size=2,  strides = 2, name='MaxPool2')(conv1)\n","    # print('conv1 m.p shape: ',conv1.shape)\n","\n","    conv2 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2], kernel_initializer=initializer, \n","                          padding='valid', activation=a_func, name='Conv3')(conv1)\n","    print('conv2 shape: ',conv2.shape)\n","    # conv2 = MaxPooling1D(pool_size=2, strides = 2, name='MaxPool3')(conv2)\n","    # print('conv2 m.p shape: ',conv2.shape)\n","\n","    conv3 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[3], kernel_initializer=initializer, \n","                          padding='valid', activation=a_func, name='Conv4')(conv2)\n","    print('conv3 shape: ',conv3.shape)\n","    # conv3 = MaxPooling1D(pool_size=2, strides = 2, name='MaxPool4')(conv3)\n","    # print('conv3 m.p shape: ',conv3.shape)\n","\n","    # conv4 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[4], kernel_initializer=initializer,\n","    #                       padding='valid', activation=a_func, name='Conv5')(conv3)\n","    # print('conv4 shape: ',conv4.shape)\n","    # conv4 = MaxPooling1D(pool_size=2, strides = 2, name='MaxPool5')(conv4)\n","    # print('conv4 m.p shape: ',conv4.shape)\n","\n","    conv5 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[5], kernel_initializer=initializer,\n","                          padding='valid', activation=a_func,  name='Conv6')(conv3)\n","    print('conv5 shape: ',conv5.shape)\n","\n","    conv5 = MaxPooling1D(pool_size=2, strides = 2, name='MaxPool6')(conv5)\n","    print('conv5 m.p shape: ',conv5.shape)\n","\n","    # k = 40\n","    # # K-max pooling\n","    # def kmax_outshape(in_shape):\n","    #     return (in_shape[0], in_shape[2]*k)\n","    # def KMaxPooling(inputs):        \n","    #     # swap last two dimensions since top_k will be applied along the last dimension\n","    #     shifted_input = tf.transpose(inputs, [0, 2, 1])\n","    #     # extract top_k, returns two tensors [values, indices]\n","    #     top_k = tf.nn.top_k(shifted_input, k=k, sorted=True, name='TopK')[0]\n","    #     return top_k\n","\n","    # conv5 = Lambda(KMaxPooling, output_shape=kmax_outshape)(conv5)\n","    conv5 = Flatten()(conv5)\n","    print('conv5 flatten shape: ',conv5.shape)\n","\n","    # Two dense layers with dropout of .5\n","    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n","    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n","\n","    # Output dense layer with softmax activation\n","    pred = Dense(cat_output, activation='softmax', name='output')(z)\n","\n","    model = Model(inputs=inputs, outputs=pred)\n","    print(model.summary())\n","    sgd = SGD(lr=0.01, momentum=0.9)\n","    adam = Adam(lr=0.001)  # Feel free to use SGD above. I found Adam with lr=0.001 is faster than SGD with lr=0.01\n","    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', f1_m ,precision_m, recall_m])\n","    # model.compile(loss= [focal_loss] ,optimizer=adam, metrics=['accuracy',  f1_m ,precision_m, recall_m])\n","    # model.compile(loss=focal_loss(alpha=1),optimizer='nadam',metrics=['accuracy'])\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krnQBIu6xUyn","colab_type":"text"},"source":["## Making Data Model Ready: Encoding Tweets"]},{"cell_type":"markdown","metadata":{"id":"Ba4weCmHlNgw","colab_type":"text"},"source":["### Quantization  "]},{"cell_type":"code","metadata":{"id":"basXFKCelM4e","colab_type":"code","colab":{}},"source":["def encode_data(x, maxlen, vocab):\n","    # Iterate over the loaded data and create a matrix of size (len(x), maxlen)\n","    # Each character is encoded into a one-hot array later at the lambda layer.\n","    # Chars not in the vocab are encoded as -1, into an all zero vector.\n","    \n","    input_data = np.zeros((len(x), maxlen), dtype=np.int)\n","    for dix, sent in enumerate(x):\n","        counter = 0\n","        for c in sent:\n","            if counter >= maxlen:\n","                pass\n","            else:\n","                ix = vocab.get(c, -1)  # get index from vocab dictionary, if not in vocab, return -1\n","                input_data[dix, counter] = ix\n","                counter += 1\n","    return input_data\n","\n","\n","def create_vocab_set():\n","    # This alphabet is 69 chars vs. 70 reported in the paper since they include two\n","    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n","\n","    alphabet = set(list(string.ascii_lowercase) + list(string.digits) +\n","                   list(string.punctuation) + ['\\n'])\n","    vocab_size = len(alphabet)\n","    vocab = {}\n","    reverse_vocab = {}\n","    for ix, t in enumerate(alphabet):\n","        vocab[t] = ix\n","        reverse_vocab[ix] = t\n","\n","    return vocab, reverse_vocab, vocab_size, alphabet\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzh6vExltua","colab_type":"code","colab":{}},"source":["vocab, reverse_vocab, vocab_size, alphabet = create_vocab_set()\n","\n","# Maximum encoding length. Longer gets chopped. Shorter gets padded.\n","max_encode_len = 1500"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9sXY-xoXJJT8","colab_type":"text"},"source":["## feature sets"]},{"cell_type":"code","metadata":{"id":"XtvD_NBu92Y9","colab_type":"code","outputId":"79b9d05d-14a8-41c5-aa26-40539d208377","executionInfo":{"status":"ok","timestamp":1573484484378,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["f1 = encode_data(X_train['new_tweet'], max_encode_len, vocab)\n","f2 = X_train['user_mentions'].values.reshape(-1,1)\n","f3 = X_train['n_hash_tags'].values.reshape(-1,1)\n","f4 = X_train['n_urls'].values.reshape(-1,1)\n","f5 = X_train['n_emojis'].values.reshape(-1,1)\n","f6 = X_train['new_tweet_length'].values.reshape(-1,1)\n","f7 = X_train['original_tweet_length'].values.reshape(-1,1)\n","\n","# X_train_bow = np.concatenate((f1 , f2 , f3 , f4 , f5 , f6 , f7 ) , axis = 1)\n","X_train_bow = np.concatenate((f1 , f2 , f3 , f4 ,  f7 ) , axis = 1)\n","\n","print(X_train_bow.shape)\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(9268, 1504)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QYVZGRQQ9ukb","colab_type":"code","outputId":"f27fa441-a555-4419-ab99-fef4d619e877","executionInfo":{"status":"ok","timestamp":1573484485845,"user_tz":-330,"elapsed":1328,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["f1 = encode_data(X_test['new_tweet'], max_encode_len, vocab)\n","f2 = X_test['user_mentions'].values.reshape(-1,1)\n","f3 = X_test['n_hash_tags'].values.reshape(-1,1)\n","f4 = X_test['n_urls'].values.reshape(-1,1)\n","f5 = X_test['n_emojis'].values.reshape(-1,1)\n","f6 = X_test['new_tweet_length'].values.reshape(-1,1)\n","f7 = X_test['original_tweet_length'].values.reshape(-1,1)\n","\n","\n","# X_test_bow = np.concatenate((f1 , f2 , f3 , f4 , f5 , f6 , f7 ) , axis = 1)\n","X_test_bow = np.concatenate((f1 , f2 , f3 , f4 ,  f7 ) , axis = 1)\n","\n","print(X_test_bow.shape)\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(3972, 1504)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wmobhn0NoePU","colab_type":"code","outputId":"ce25cdb0-1477-49f3-f84f-b9470043b2fc","executionInfo":{"status":"ok","timestamp":1573484488752,"user_tz":-330,"elapsed":1352,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["Y_train_cat  = to_categorical(Y_train) \n","Y_test_cat = to_categorical(Y_test)\n","\n","class_weight = {1:  1 - Y_train.sum()/Y_train.shape[0] , 0 : Y_train.sum()/Y_train.shape[0] }\n","print(\"Class_weights : \" ,  class_weight)\n","\n","print(X_train_bow.shape , Y_train.shape , Y_train_cat.shape)\n","print(X_test_bow.shape , Y_test.shape ,  Y_test_cat.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Class_weights :  {1: 0.6676737160120846, 0: 0.3323262839879154}\n","(9268, 1504) (9268,) (9268, 2)\n","(3972, 1504) (3972,) (3972, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pan5gy153kAD","colab_type":"text"},"source":["## Using k-fold"]},{"cell_type":"code","metadata":{"id":"PP1VY1GKCwGQ","colab_type":"code","colab":{}},"source":["np.random.seed(123)  # for reproducibility\n","save = False\n","model_name_path = root_path + 'CNN/A_params/crepe_model.json'\n","model_weights_path = root_path + 'CNN/A_params/crepe_model_weights.h5'\n","\n","# Model params\n","# Maximum encoding length. Longer gets chopped. Shorter gets padded.\n","maxlen = X_train_bow.shape[1]\n","# Filters for conv layers\n","nb_filter = 512\n","# Number of units in the dense layer\n","dense_outputs = 1024\n","# Conv layer kernel size\n","filter_kernels = [7,5,3,2,2,1]\n","# Number of units in the final output layer. Number of classes.\n","cat_output = 2\n","\n","# Compile/fit params\n","batch_size = 80\n","nb_epoch = 20\n","\n","alphabet = set(list(string.ascii_lowercase) + list(string.digits) + list(string.punctuation) + ['\\n'])\n","vocab_size =  len(alphabet)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ytnf6WdehNk5","colab_type":"code","outputId":"9e031989-166b-442d-d68f-c3ea1447583a","executionInfo":{"status":"ok","timestamp":1573466215734,"user_tz":-330,"elapsed":636840,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","\n","seed = 10\n","fold = 1\n","\n","kfold = StratifiedKFold(n_splits=3)\n","\n","# kfold.get_n_splits(X_train_bow)\n","cvscores = []\n","testscores = []\n","test_predictions = []\n","\n","for train, val in kfold.split(X_train_bow  , Y_train):\n","    print(\"------------------------------------------------------------------------------\")\n","    print(\"FOLD \", fold)\n","\n","    model = create_model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output)\n","\n","    model.fit( X_train_bow[train] , Y_train_cat[train] ,class_weight = class_weight , batch_size=batch_size, epochs=nb_epoch, validation_split=0.0)\n","    \n","    val_score = model.evaluate( X_train_bow[val], Y_train_cat[val] , verbose=0)\n","    test_score = model.evaluate( X_test_bow , Y_test_cat , verbose=0 )\n","    temp_pred = model.predict(X_test_bow)\n","    test_predictions.append(temp_pred.argmax(axis = 1))\n","    \n","    print(\"\\n =====   Fold ==== \" ,  fold , \" : \" , val_score[1])\n","    \n","    cvscores.append(val_score[1]*100)\n","    testscores.append(test_score[1]*100)\n","\n","    fold += 1\n","\n","print(\"------------------------------------------------------------------------------\")\n","print(\"Validation Accuracy : %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n","print(\"Test Accuracy : %.2f%% (+/- %.2f%%)\" % (np.mean(testscores), np.std(testscores)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["------------------------------------------------------------------------------\n","FOLD  1\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","inputs shape:  (?, 1504)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","conv shape:  (?, 1498, 512)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","conv max pool shape:  (?, 749, 512)\n","conv1 shape:  (?, 745, 512)\n","conv2 shape:  (?, 743, 512)\n","conv3 shape:  (?, 742, 512)\n","conv5 shape:  (?, 742, 512)\n","conv5 m.p shape:  (?, 371, 512)\n","conv5 flatten shape:  (?, ?)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 1504)              0         \n","_________________________________________________________________\n","lambda_1 (Lambda)            (None, 1504, 69)          0         \n","_________________________________________________________________\n","Conv1 (Conv1D)               (None, 1498, 512)         247808    \n","_________________________________________________________________\n","MaxPool1 (MaxPooling1D)      (None, 749, 512)          0         \n","_________________________________________________________________\n","Conv2 (Conv1D)               (None, 745, 512)          1311232   \n","_________________________________________________________________\n","Conv3 (Conv1D)               (None, 743, 512)          786944    \n","_________________________________________________________________\n","Conv4 (Conv1D)               (None, 742, 512)          524800    \n","_________________________________________________________________\n","Conv6 (Conv1D)               (None, 742, 512)          262656    \n","_________________________________________________________________\n","MaxPool6 (MaxPooling1D)      (None, 371, 512)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 189952)            0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1024)              194511872 \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1024)              1049600   \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 2)                 2050      \n","=================================================================\n","Total params: 198,696,962\n","Trainable params: 198,696,962\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/20\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","6178/6178 [==============================] - 73s 12ms/step - loss: 0.3156 - acc: 0.5133 - f1_m: 0.5133 - precision_m: 0.5133 - recall_m: 0.5133\n","Epoch 2/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.3075 - acc: 0.5596 - f1_m: 0.5596 - precision_m: 0.5596 - recall_m: 0.5596\n","Epoch 3/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.3066 - acc: 0.5495 - f1_m: 0.5495 - precision_m: 0.5495 - recall_m: 0.5495\n","Epoch 4/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3060 - acc: 0.5636 - f1_m: 0.5636 - precision_m: 0.5636 - recall_m: 0.5636\n","Epoch 5/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.3053 - acc: 0.5497 - f1_m: 0.5497 - precision_m: 0.5497 - recall_m: 0.5497\n","Epoch 6/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.3050 - acc: 0.5688 - f1_m: 0.5688 - precision_m: 0.5688 - recall_m: 0.5688\n","Epoch 7/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3032 - acc: 0.5751 - f1_m: 0.5751 - precision_m: 0.5751 - recall_m: 0.5751\n","Epoch 8/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3017 - acc: 0.5861 - f1_m: 0.5861 - precision_m: 0.5861 - recall_m: 0.5861\n","Epoch 9/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2987 - acc: 0.5984 - f1_m: 0.5984 - precision_m: 0.5984 - recall_m: 0.5984\n","Epoch 10/20\n","6178/6178 [==============================] - 65s 10ms/step - loss: 0.2929 - acc: 0.6293 - f1_m: 0.6293 - precision_m: 0.6293 - recall_m: 0.6293\n","Epoch 11/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.2817 - acc: 0.6669 - f1_m: 0.6669 - precision_m: 0.6669 - recall_m: 0.6669\n","Epoch 12/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2660 - acc: 0.6946 - f1_m: 0.6946 - precision_m: 0.6946 - recall_m: 0.6946\n","Epoch 13/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.2412 - acc: 0.7493 - f1_m: 0.7493 - precision_m: 0.7493 - recall_m: 0.7493\n","Epoch 14/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2058 - acc: 0.8003 - f1_m: 0.8003 - precision_m: 0.8003 - recall_m: 0.8003\n","Epoch 15/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.1418 - acc: 0.8792 - f1_m: 0.8792 - precision_m: 0.8792 - recall_m: 0.8792\n","Epoch 16/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0921 - acc: 0.9317 - f1_m: 0.9317 - precision_m: 0.9317 - recall_m: 0.9317\n","Epoch 17/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0651 - acc: 0.9518 - f1_m: 0.9518 - precision_m: 0.9518 - recall_m: 0.9518\n","Epoch 18/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0415 - acc: 0.9735 - f1_m: 0.9735 - precision_m: 0.9735 - recall_m: 0.9735\n","Epoch 19/20\n","6178/6178 [==============================] - 66s 11ms/step - loss: 0.0299 - acc: 0.9824 - f1_m: 0.9824 - precision_m: 0.9824 - recall_m: 0.9824\n","Epoch 20/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0243 - acc: 0.9864 - f1_m: 0.9864 - precision_m: 0.9864 - recall_m: 0.9864\n","\n"," =====   Fold ====  1  :  0.6323624596240837\n","------------------------------------------------------------------------------\n","FOLD  2\n","inputs shape:  (?, 1504)\n","conv shape:  (?, 1498, 512)\n","conv max pool shape:  (?, 749, 512)\n","conv1 shape:  (?, 745, 512)\n","conv2 shape:  (?, 743, 512)\n","conv3 shape:  (?, 742, 512)\n","conv5 shape:  (?, 742, 512)\n","conv5 m.p shape:  (?, 371, 512)\n","conv5 flatten shape:  (?, ?)\n","Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 1504)              0         \n","_________________________________________________________________\n","lambda_2 (Lambda)            (None, 1504, 69)          0         \n","_________________________________________________________________\n","Conv1 (Conv1D)               (None, 1498, 512)         247808    \n","_________________________________________________________________\n","MaxPool1 (MaxPooling1D)      (None, 749, 512)          0         \n","_________________________________________________________________\n","Conv2 (Conv1D)               (None, 745, 512)          1311232   \n","_________________________________________________________________\n","Conv3 (Conv1D)               (None, 743, 512)          786944    \n","_________________________________________________________________\n","Conv4 (Conv1D)               (None, 742, 512)          524800    \n","_________________________________________________________________\n","Conv6 (Conv1D)               (None, 742, 512)          262656    \n","_________________________________________________________________\n","MaxPool6 (MaxPooling1D)      (None, 371, 512)          0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 189952)            0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1024)              194511872 \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1024)              1049600   \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 2)                 2050      \n","=================================================================\n","Total params: 198,696,962\n","Trainable params: 198,696,962\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/20\n","6178/6178 [==============================] - 69s 11ms/step - loss: 0.3189 - acc: 0.5076 - f1_m: 0.5076 - precision_m: 0.5076 - recall_m: 0.5076\n","Epoch 2/20\n","6178/6178 [==============================] - 65s 10ms/step - loss: 0.3083 - acc: 0.5453 - f1_m: 0.5453 - precision_m: 0.5453 - recall_m: 0.5453\n","Epoch 3/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3060 - acc: 0.5539 - f1_m: 0.5539 - precision_m: 0.5539 - recall_m: 0.5539\n","Epoch 4/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3062 - acc: 0.5489 - f1_m: 0.5489 - precision_m: 0.5489 - recall_m: 0.5489\n","Epoch 5/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3056 - acc: 0.5513 - f1_m: 0.5513 - precision_m: 0.5513 - recall_m: 0.5513\n","Epoch 6/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3049 - acc: 0.5594 - f1_m: 0.5594 - precision_m: 0.5594 - recall_m: 0.5594\n","Epoch 7/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3040 - acc: 0.5625 - f1_m: 0.5625 - precision_m: 0.5625 - recall_m: 0.5625\n","Epoch 8/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3024 - acc: 0.5837 - f1_m: 0.5837 - precision_m: 0.5837 - recall_m: 0.5837\n","Epoch 9/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.3009 - acc: 0.5835 - f1_m: 0.5835 - precision_m: 0.5835 - recall_m: 0.5835\n","Epoch 10/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2972 - acc: 0.6154 - f1_m: 0.6154 - precision_m: 0.6154 - recall_m: 0.6154\n","Epoch 11/20\n","6178/6178 [==============================] - 65s 10ms/step - loss: 0.2895 - acc: 0.6496 - f1_m: 0.6496 - precision_m: 0.6496 - recall_m: 0.6496\n","Epoch 12/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2732 - acc: 0.6907 - f1_m: 0.6907 - precision_m: 0.6907 - recall_m: 0.6907\n","Epoch 13/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2518 - acc: 0.7379 - f1_m: 0.7379 - precision_m: 0.7379 - recall_m: 0.7379\n","Epoch 14/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.2141 - acc: 0.7915 - f1_m: 0.7915 - precision_m: 0.7915 - recall_m: 0.7915\n","Epoch 15/20\n","6178/6178 [==============================] - 65s 10ms/step - loss: 0.1618 - acc: 0.8613 - f1_m: 0.8613 - precision_m: 0.8613 - recall_m: 0.8613\n","Epoch 16/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.1087 - acc: 0.9095 - f1_m: 0.9095 - precision_m: 0.9095 - recall_m: 0.9095\n","Epoch 17/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0760 - acc: 0.9421 - f1_m: 0.9421 - precision_m: 0.9421 - recall_m: 0.9421\n","Epoch 18/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0487 - acc: 0.9628 - f1_m: 0.9628 - precision_m: 0.9628 - recall_m: 0.9628\n","Epoch 19/20\n","6178/6178 [==============================] - 65s 11ms/step - loss: 0.0630 - acc: 0.9485 - f1_m: 0.9485 - precision_m: 0.9485 - recall_m: 0.9485\n","Epoch 20/20\n","6178/6178 [==============================] - 65s 10ms/step - loss: 0.0447 - acc: 0.9660 - f1_m: 0.9660 - precision_m: 0.9660 - recall_m: 0.9660\n","\n"," =====   Fold ====  2  :  0.5902912621359223\n","------------------------------------------------------------------------------\n","FOLD  3\n","inputs shape:  (?, 1504)\n","conv shape:  (?, 1498, 512)\n","conv max pool shape:  (?, 749, 512)\n","conv1 shape:  (?, 745, 512)\n","conv2 shape:  (?, 743, 512)\n","conv3 shape:  (?, 742, 512)\n","conv5 shape:  (?, 742, 512)\n","conv5 m.p shape:  (?, 371, 512)\n","conv5 flatten shape:  (?, ?)\n","Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         (None, 1504)              0         \n","_________________________________________________________________\n","lambda_3 (Lambda)            (None, 1504, 69)          0         \n","_________________________________________________________________\n","Conv1 (Conv1D)               (None, 1498, 512)         247808    \n","_________________________________________________________________\n","MaxPool1 (MaxPooling1D)      (None, 749, 512)          0         \n","_________________________________________________________________\n","Conv2 (Conv1D)               (None, 745, 512)          1311232   \n","_________________________________________________________________\n","Conv3 (Conv1D)               (None, 743, 512)          786944    \n","_________________________________________________________________\n","Conv4 (Conv1D)               (None, 742, 512)          524800    \n","_________________________________________________________________\n","Conv6 (Conv1D)               (None, 742, 512)          262656    \n","_________________________________________________________________\n","MaxPool6 (MaxPooling1D)      (None, 371, 512)          0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 189952)            0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1024)              194511872 \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 1024)              1049600   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 2)                 2050      \n","=================================================================\n","Total params: 198,696,962\n","Trainable params: 198,696,962\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/20\n","6180/6180 [==============================] - 69s 11ms/step - loss: 0.3233 - acc: 0.5189 - f1_m: 0.5189 - precision_m: 0.5189 - recall_m: 0.5189\n","Epoch 2/20\n","6180/6180 [==============================] - 66s 11ms/step - loss: 0.3068 - acc: 0.5542 - f1_m: 0.5542 - precision_m: 0.5542 - recall_m: 0.5542\n","Epoch 3/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.3054 - acc: 0.5618 - f1_m: 0.5618 - precision_m: 0.5618 - recall_m: 0.5618\n","Epoch 4/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.3058 - acc: 0.5576 - f1_m: 0.5576 - precision_m: 0.5576 - recall_m: 0.5576\n","Epoch 5/20\n","6180/6180 [==============================] - 65s 10ms/step - loss: 0.3053 - acc: 0.5589 - f1_m: 0.5589 - precision_m: 0.5589 - recall_m: 0.5589\n","Epoch 6/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.3039 - acc: 0.5694 - f1_m: 0.5694 - precision_m: 0.5694 - recall_m: 0.5694\n","Epoch 7/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.3030 - acc: 0.5772 - f1_m: 0.5772 - precision_m: 0.5772 - recall_m: 0.5772\n","Epoch 8/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.3012 - acc: 0.5953 - f1_m: 0.5953 - precision_m: 0.5953 - recall_m: 0.5953\n","Epoch 9/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.2981 - acc: 0.6031 - f1_m: 0.6031 - precision_m: 0.6031 - recall_m: 0.6031\n","Epoch 10/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.2915 - acc: 0.6476 - f1_m: 0.6476 - precision_m: 0.6476 - recall_m: 0.6476\n","Epoch 11/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.2813 - acc: 0.6704 - f1_m: 0.6704 - precision_m: 0.6704 - recall_m: 0.6704\n","Epoch 12/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.2664 - acc: 0.7013 - f1_m: 0.7013 - precision_m: 0.7013 - recall_m: 0.7013\n","Epoch 13/20\n","6180/6180 [==============================] - 65s 10ms/step - loss: 0.2316 - acc: 0.7676 - f1_m: 0.7676 - precision_m: 0.7676 - recall_m: 0.7676\n","Epoch 14/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.1876 - acc: 0.8249 - f1_m: 0.8249 - precision_m: 0.8249 - recall_m: 0.8249\n","Epoch 15/20\n","6180/6180 [==============================] - 65s 10ms/step - loss: 0.1443 - acc: 0.8754 - f1_m: 0.8754 - precision_m: 0.8754 - recall_m: 0.8754\n","Epoch 16/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.1011 - acc: 0.9172 - f1_m: 0.9172 - precision_m: 0.9172 - recall_m: 0.9172\n","Epoch 17/20\n","6180/6180 [==============================] - 65s 10ms/step - loss: 0.0608 - acc: 0.9555 - f1_m: 0.9555 - precision_m: 0.9555 - recall_m: 0.9555\n","Epoch 18/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.0680 - acc: 0.9430 - f1_m: 0.9430 - precision_m: 0.9430 - recall_m: 0.9430\n","Epoch 19/20\n","6180/6180 [==============================] - 65s 10ms/step - loss: 0.0335 - acc: 0.9757 - f1_m: 0.9757 - precision_m: 0.9757 - recall_m: 0.9757\n","Epoch 20/20\n","6180/6180 [==============================] - 65s 11ms/step - loss: 0.0256 - acc: 0.9824 - f1_m: 0.9824 - precision_m: 0.9824 - recall_m: 0.9824\n","\n"," =====   Fold ====  3  :  0.6036269430051814\n","------------------------------------------------------------------------------\n","Validation Accuracy : 60.88% (+/- 1.76%)\n","Test Accuracy : 60.93% (+/- 1.57%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"en53n1zzJwDe","colab_type":"code","outputId":"07a55a63-45dd-44aa-fb1d-9d601996dc39","executionInfo":{"status":"ok","timestamp":1573466239567,"user_tz":-330,"elapsed":1241,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["# if sum is (0 or 1) -> 0(predicted class )  else (2 , 3) -> 1\n","# final_prediction = np.sum(np.array(test_predictions).T , axis = 1)//2\n","from scipy import stats\n","\n","final_prediction = stats.mode(np.array(test_predictions).T,axis=1)[0].flatten()\n","print(classification_report(Y_test.values , final_prediction))\n","results = confusion_matrix(Y_test.values , final_prediction)\n","\n","%matplotlib inline\n","plt.figure(figsize = (5,5))\n","ax = sns.heatmap(results, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14},fmt='g')\n","bottom, top = ax.get_ylim()\n","ax.set_ylim(bottom + 0.5, top - 0.5)\n","plt.ylabel('True Class')\n","plt.xlabel('Predicted Class')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.80      0.74      2652\n","           1       0.43      0.30      0.35      1320\n","\n","    accuracy                           0.63      3972\n","   macro avg       0.56      0.55      0.55      3972\n","weighted avg       0.61      0.63      0.61      3972\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUgAAAE9CAYAAABp+/tBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCTE/wN5PWsmJ9uwbgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 360x360 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"vBH_y6RukNgY","colab_type":"code","outputId":"7cc3a1c3-0292-4ac7-8b10-25c1e4d8199a","executionInfo":{"status":"ok","timestamp":1573466283295,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"Y_test: \",Y_test.values)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Y_test:  [0 0 0 ... 1 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JOtS7gupkQiH","colab_type":"code","outputId":"c880312c-4f3c-495b-cbeb-b111313c7001","executionInfo":{"status":"ok","timestamp":1573466283297,"user_tz":-330,"elapsed":816,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"final_prediction: \",final_prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["final_prediction:  [0 0 0 ... 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3q68YbN53oy9","colab_type":"code","outputId":"6aab23aa-303c-48ea-920a-102306a21851","executionInfo":{"status":"ok","timestamp":1573466285986,"user_tz":-330,"elapsed":1315,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["f1_score(Y_test, final_prediction, average='weighted')  "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6132258159775238"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"xTQi0OLM3q8R","colab_type":"text"},"source":["## Without k-fold"]},{"cell_type":"code","metadata":{"id":"6VQZDmZG91lB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"7d787a41-ff37-455c-e013-04755eb5985f","executionInfo":{"status":"ok","timestamp":1573484507997,"user_tz":-330,"elapsed":1095,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["X_final_data = np.concatenate((X_train_bow, X_test_bow) , axis = 0)\n","Y_final_data = np.concatenate((Y_train , Y_test) ,axis = 0)\n","Y_final_data_cat = to_categorical(Y_final_data)\n","print(X_final_data.shape)\n","print(Y_final_data.shape)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["(13240, 1504)\n","(13240,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q1dTqJUDCJN1","colab_type":"code","outputId":"1e5bba8d-a42e-4b8d-95d3-9415bfc75cc5","executionInfo":{"status":"ok","timestamp":1573487605824,"user_tz":-330,"elapsed":2945145,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = create_model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output)\n","model.fit( X_final_data , Y_final_data_cat , class_weight =  class_weight , batch_size=batch_size, epochs=nb_epoch, validation_split=0.0)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["inputs shape:  (?, 1504)\n","conv shape:  (?, 1498, 512)\n","conv max pool shape:  (?, 749, 512)\n","conv1 shape:  (?, 745, 512)\n","conv2 shape:  (?, 743, 512)\n","conv3 shape:  (?, 742, 512)\n","conv5 shape:  (?, 742, 512)\n","conv5 m.p shape:  (?, 371, 512)\n","conv5 flatten shape:  (?, ?)\n","Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 1504)              0         \n","_________________________________________________________________\n","lambda_2 (Lambda)            (None, 1504, 69)          0         \n","_________________________________________________________________\n","Conv1 (Conv1D)               (None, 1498, 512)         247808    \n","_________________________________________________________________\n","MaxPool1 (MaxPooling1D)      (None, 749, 512)          0         \n","_________________________________________________________________\n","Conv2 (Conv1D)               (None, 745, 512)          1311232   \n","_________________________________________________________________\n","Conv3 (Conv1D)               (None, 743, 512)          786944    \n","_________________________________________________________________\n","Conv4 (Conv1D)               (None, 742, 512)          524800    \n","_________________________________________________________________\n","Conv6 (Conv1D)               (None, 742, 512)          262656    \n","_________________________________________________________________\n","MaxPool6 (MaxPooling1D)      (None, 371, 512)          0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 189952)            0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1024)              194511872 \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1024)              1049600   \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 2)                 2050      \n","=================================================================\n","Total params: 198,696,962\n","Trainable params: 198,696,962\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/20\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","13240/13240 [==============================] - 160s 12ms/step - loss: 0.3160 - acc: 0.5264 - f1_m: 0.5264 - precision_m: 0.5264 - recall_m: 0.5264\n","Epoch 2/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.3068 - acc: 0.5462 - f1_m: 0.5462 - precision_m: 0.5462 - recall_m: 0.5462\n","Epoch 3/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.3063 - acc: 0.5530 - f1_m: 0.5530 - precision_m: 0.5530 - recall_m: 0.5530\n","Epoch 4/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.3056 - acc: 0.5576 - f1_m: 0.5576 - precision_m: 0.5576 - recall_m: 0.5576\n","Epoch 5/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.3056 - acc: 0.5561 - f1_m: 0.5561 - precision_m: 0.5561 - recall_m: 0.5561\n","Epoch 6/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.3047 - acc: 0.5595 - f1_m: 0.5595 - precision_m: 0.5595 - recall_m: 0.5595\n","Epoch 7/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.3028 - acc: 0.5758 - f1_m: 0.5758 - precision_m: 0.5758 - recall_m: 0.5758\n","Epoch 8/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.2993 - acc: 0.5965 - f1_m: 0.5965 - precision_m: 0.5965 - recall_m: 0.5965\n","Epoch 9/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.2941 - acc: 0.6091 - f1_m: 0.6091 - precision_m: 0.6091 - recall_m: 0.6091\n","Epoch 10/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.2822 - acc: 0.6557 - f1_m: 0.6557 - precision_m: 0.6557 - recall_m: 0.6557\n","Epoch 11/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.2618 - acc: 0.6983 - f1_m: 0.6983 - precision_m: 0.6983 - recall_m: 0.6983\n","Epoch 12/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.2275 - acc: 0.7682 - f1_m: 0.7682 - precision_m: 0.7682 - recall_m: 0.7682\n","Epoch 13/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.1798 - acc: 0.8267 - f1_m: 0.8267 - precision_m: 0.8267 - recall_m: 0.8267\n","Epoch 14/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.1250 - acc: 0.8887 - f1_m: 0.8887 - precision_m: 0.8887 - recall_m: 0.8887\n","Epoch 15/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.0978 - acc: 0.9153 - f1_m: 0.9153 - precision_m: 0.9153 - recall_m: 0.9153\n","Epoch 16/20\n","13240/13240 [==============================] - 147s 11ms/step - loss: 0.0871 - acc: 0.9261 - f1_m: 0.9261 - precision_m: 0.9261 - recall_m: 0.9261\n","Epoch 17/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.0675 - acc: 0.9473 - f1_m: 0.9473 - precision_m: 0.9473 - recall_m: 0.9473\n","Epoch 18/20\n","13240/13240 [==============================] - 145s 11ms/step - loss: 0.0621 - acc: 0.9511 - f1_m: 0.9511 - precision_m: 0.9511 - recall_m: 0.9511\n","Epoch 19/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.0497 - acc: 0.9622 - f1_m: 0.9622 - precision_m: 0.9622 - recall_m: 0.9622\n","Epoch 20/20\n","13240/13240 [==============================] - 146s 11ms/step - loss: 0.0414 - acc: 0.9678 - f1_m: 0.9678 - precision_m: 0.9678 - recall_m: 0.9678\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fdcd68b7cc0>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"G0yqOcw-BkDz","colab_type":"code","colab":{}},"source":["y_test_predict = model.predict(X_test_bow)\n","y_test_predict_cat = y_test_predict.argmax(axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8H-z2Nv3etP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":513},"outputId":"95c06713-0ca8-4797-d4cf-a0dc3a8d01a9","executionInfo":{"status":"ok","timestamp":1573487623384,"user_tz":-330,"elapsed":17549,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["print(classification_report(Y_test.values , y_test_predict_cat))\n","results = confusion_matrix(Y_test.values , y_test_predict_cat)\n","\n","%matplotlib inline\n","plt.figure(figsize = (5,5))\n","ax = sns.heatmap(results, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14},fmt='g')\n","bottom, top = ax.get_ylim()\n","ax.set_ylim(bottom + 0.5, top - 0.5)\n","plt.ylabel('True Class')\n","plt.xlabel('Predicted Class')\n","plt.show()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99      2652\n","           1       0.99      0.97      0.98      1320\n","\n","    accuracy                           0.99      3972\n","   macro avg       0.99      0.98      0.98      3972\n","weighted avg       0.99      0.99      0.99      3972\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUgAAAE9CAYAAABp+/tBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJqZpfg/Tw2blCVrIyUAAAAASUVORK5CYII=\n","text/plain":["<Figure size 360x360 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UX4y4Gge3vra","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"802fdb26-fc04-4cb5-9690-8ea3eb799fd7","executionInfo":{"status":"ok","timestamp":1573487623384,"user_tz":-330,"elapsed":17544,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["f1_score(Y_test, y_test_predict_cat , average='weighted')  "],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9863730904329313"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-70igbqVsf-A"},"source":["### Misclassified examples"]},{"cell_type":"code","metadata":{"id":"yl6cb8_8ynqL","colab_type":"code","colab":{}},"source":["y_test = np.asarray(Y_test)\n","misclassified = np.where(y_test != y_test_predict_cat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OUkFMPY0Jk7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"c6962e44-ba53-4992-d305-4a1d6fbabcb5","executionInfo":{"status":"ok","timestamp":1573487623386,"user_tz":-330,"elapsed":17535,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["# print(type(misclassified))\n","print(len(misclassified[0]), 'misclassifications')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["54 misclassifications\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PZXpM9q2nX3T","colab_type":"code","colab":{}},"source":["def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \n","def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk)) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikBh1MoTpXjQ","colab_type":"text"},"source":["Red: original not offensive (0), classified as offensive (1) <br>\n","Green: original offensive (1), classified as not offensive (0)"]},{"cell_type":"code","metadata":{"id":"9WDmEpSy0dX8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":996},"outputId":"2a8a95db-337b-48d1-f0b8-26c94ce31b27","executionInfo":{"status":"ok","timestamp":1573487623388,"user_tz":-330,"elapsed":17523,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["#OFF=1, NOT=0)\n","for index in misclassified[0]:\n","  #original not offensive (0), classified as offensive (1) \n","  if y_test[index] == 0 and y_test_predict_cat[index] == 1:\n","    prRed(X_test.iloc[index,0])\n","  #original offensive (1), classified as not offensive (0)\n","  elif y_test[index] == 1 and y_test_predict_cat[index] == 0:\n","    prGreen(X_test.iloc[index,0])"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\u001b[92m @USER I was think she jisoo but I’m Focused I found she is taeyoun\u001b[00m\n","\u001b[92m @USER Fool\u001b[00m\n","\u001b[92m @USER @USER WHAT A DOG..SHE IS...\u001b[00m\n","\u001b[92m @USER Clown\u001b[00m\n","\u001b[91m @USER I blame @USER for inciting violence and she should be held accountable! She is a despicable disgrace.\u001b[00m\n","\u001b[92m @USER @USER Popular side doesn’t equal winning side. Facts are that private schools do a much better job educating students than public schools. Public school teachers get unfair perks that they don’t deserve. It’s unfair to private school teachers.\u001b[00m\n","\u001b[92m @USER Then I see like this and lust over the girl with the long blond hair and the bare midriff and micro-mini and realize she is now 48 years older and could be a great grandmother today! 😱 People wonder why I love legs until I show them what I grew up with. URL\u001b[00m\n","\u001b[92m @USER Looks Like The Jokes On Liberals Again.  #FortTrump #Poland #BoomingEconomy URL\u001b[00m\n","\u001b[92m @USER And I will never watch your movies again because you are a maniac!\u001b[00m\n","\u001b[92m @USER @USER @USER I have been thinking about it and it's not inconceivable that someone was trying to meddle with people's minds regarding gun control. I am double-minded about the use of arms because I am a peacemaker. Shoot once and you could start a big war. &gt;2\u001b[00m\n","\u001b[92m @USER @USER Rescue conservatives from its own demons? Who are the two or three most notorious serial abusers of women in the last 30 years? Hint: they were not GOP. Did they ever face consequences?\u001b[00m\n","\u001b[92m @USER @USER David stop been a mouthpiece for the anti gun people you r nothing but a worm looking for fame now remember that antifa BLM and all the radical left groups are violent and hurt people too but of course you agree with them\u001b[00m\n","\u001b[92m @USER @USER SERVES THE BASTARDS RIGHT!\u001b[00m\n","\u001b[91m @USER I would be surprised if she is even on tomorrow\u001b[00m\n","\u001b[92m @USER I knew you weren’t a Real Republican committed to your party!!!! You need to leave. Get out now!! You are a disgrace to every republican/conservative member who work towards solutions and does not constantly diminish The POTUS!!! Quit already.\u001b[00m\n","\u001b[91m @USER Oh yes!  Gaddafi was a real Saint.  At least in Trump World he is. Ya know there is probably a viable argument for that now that I think about it. 😂\u001b[00m\n","\u001b[91m @USER @USER This is liberals for ya.\u001b[00m\n","\u001b[92m @USER @USER @USER @USER Wrong...We are sending another giant F....you to Liberals...😉\u001b[00m\n","\u001b[92m @USER IK never did. He is liar.\u001b[00m\n","\u001b[91m @USER @USER Nothing more satisfying than a good scratch (or “howk” if you are a scottish badger!)\u001b[00m\n","\u001b[91m @USER Bullshit.\u001b[00m\n","\u001b[92m @USER @USER @USER @USER @USER All Vince Cable has to show for getting into bed with the Tories is a ridiculous Knighthood.\u001b[00m\n","\u001b[91m @USER I find them super cool @USER I find myself pasting screen caps of them in my Discord. I hope for more in the future as it makes the game more interactive in the sense that my character feels more involved in what is going on around him. More please!\u001b[00m\n","\u001b[92m @USER @USER Mental case for sure\u001b[00m\n","\u001b[92m @USER @USER So Alyssa you saying he’s guilty?  What ever happened to innocent before guilty?  And if he is innocent would you at least tweet an apology?  Shut up and answer the tweet.\u001b[00m\n","\u001b[92m @USER @USER @USER @USER as of right now my only solid position is that the NRA has become far too influential in politics and as a result they have delayed the desire for gun control for about 20 years.\u001b[00m\n","\u001b[92m 5.10am Still a crowd at Queens Park protesting the conservatives middle of the night power grab. Shame on the conservatives! What kind of party creates chaos during an election? Rather than working with their citizens to create a healthy economy? @USER #ONpoli\u001b[00m\n","\u001b[91m @USER Liberals are dangerous. They'll say and do anything to push their agenda. The victims are judge Kavanaugh's wife and daughters.\u001b[00m\n","\u001b[92m @USER @USER She is a 🤡\u001b[00m\n","\u001b[92m @USER @USER CAREFUL now soyboy stickman. Your arms might fall clean off. Dweeb.\u001b[00m\n","\u001b[92m @USER that shit different frfr.\u001b[00m\n","\u001b[91m @USER I can't drink. URL\u001b[00m\n","\u001b[91m @USER  State Sponsored Kids Protesting the 2nd Amendment?  Youth invasion of D.C. took place as young fascists-in-training were corralled into D.C. to demand that govt strip away the liberties of all in the name of gun control  URL  '\u001b[00m\n","\u001b[91m @USER Blocked\u001b[00m\n","\u001b[91m @USER Democrats are getting everything they deserve.\u001b[00m\n","\u001b[92m @USER I used to do signifing monkey by heart. Played it for the wife and she is like what the hell?\u001b[00m\n","\u001b[92m @USER @USER Liberals are the worst sexists.  Not all women speak truth especially political activists.  Try judging the veracity of the allegation by evidence instead of gender.\u001b[00m\n","\u001b[92m @USER He makes dope ass songs. He’s a singer/writer URL\u001b[00m\n","\u001b[92m Lethargic voters get what they deserve! Aggressive voters MAGA URL\u001b[00m\n","\u001b[91m #DickDurbin is from Illinois. #Chicago has the strictest gun control and is a gang and drug killing field.\u001b[00m\n","\u001b[92m @USER   How is it you had a document that all those women signed if you didn’t know anything about the accusation. Sounds to me a cover up was planned. You are a disgrace to the office you hold\u001b[00m\n","\u001b[92m @USER Again she is the GOAT at HER SPORT but of all athletes ever? Absolutely not\u001b[00m\n","\u001b[92m @USER Down with liberals!\u001b[00m\n","\u001b[91m . @USER @USER @USER @USER Fake conservatives and their fake outrage over the budget betrayal URL URL\u001b[00m\n","\u001b[92m .She is dirty like Trump...She stole Ted Cruz campaign voters list and gave it to Trump URL\u001b[00m\n","\u001b[92m @USER @USER We all know that the left is going to believe all those lies demo liberals rats are there to destroy President Trump 👎Nov is going to be red 👍🇺🇸\u001b[00m\n","\u001b[92m @USER I really thought so. He is the God of rap. How do you come back from that\u001b[00m\n","\u001b[91m @USER Hillary is a good reason to keep the electoral college.\u001b[00m\n","\u001b[92m @USER and sometimes these edgy bitch boys are not secretly soft (good trope!!) they are just straight up.... abusive..... this is a very thin line to walk and dramione doesn't walk it well\u001b[00m\n","\u001b[92m @USER fool\u001b[00m\n","\u001b[92m @USER Creepy\u001b[00m\n","\u001b[92m @USER We are so glad we don’t know anyone like those miscreant. Pitiful bunch.\u001b[00m\n","\u001b[92m @USER He is! Talking about “ I don’t need excuses I just need my money” 💀😂\u001b[00m\n","\u001b[92m @USER @USER Miriam\u001b[00m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JChsBunODSDB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"21ee4cbf-f4a9-46a7-d57f-4226371ba613","executionInfo":{"status":"ok","timestamp":1573487638443,"user_tz":-330,"elapsed":32572,"user":{"displayName":"Archit Kumar","photoUrl":"","userId":"03572085286762964127"}}},"source":["# serialize model to YAML\n","model_yaml = model.to_yaml()\n","with open(root_path + \"/CNN/model_A.yaml\", \"w\") as yaml_file:\n","    yaml_file.write(model_yaml)\n","# serialize weights to HDF5\n","model.save_weights(root_path + \"/CNN/model_A.h5\")\n","print(\"Saved model to disk\")\n"," "],"execution_count":31,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZGEx4OuMDcck","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}